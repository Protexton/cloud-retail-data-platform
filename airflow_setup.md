# âš™ï¸ ConfiguraciÃ³n local de Apache Airflow (Simulada)

Este proyecto incluye un DAG para simular el flujo de orquestaciÃ³n de datos de ventas en un entorno retail.

## ğŸ›  Requisitos

- Python 3.8+
- Apache Airflow
- pip install apache-airflow

## ğŸ“‚ Estructura

El DAG `sales_etl_pipeline` contiene tres tareas:

1. **extract_data** â€“ Carga de datos de origen
2. **transform_data** â€“ Limpieza, joins, enriquecimiento
3. **load_data** â€“ ExportaciÃ³n al modelo final

## â–¶ï¸ CÃ³mo simular la ejecuciÃ³n

Puedes ejecutar los scripts directamente con Python para probar el flujo de datos sin desplegar un entorno Airflow completo.

Para desplegar con Airflow real:

```bash
export AIRFLOW_HOME=~/airflow
airflow db init
airflow users create --username iker --firstname Iker --lastname Reina --role Admin --email iker@example.com
airflow webserver -p 8080
airflow scheduler
```

Y luego cargar el DAG desde la carpeta `/dags`.

